{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32de85da-7995-424c-b8e7-35d498d5983c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (4.51.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: accelerate in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (1.6.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: evaluate in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (0.4.3)\n",
      "Requirement already satisfied: jiwer in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (3.1.0)\n",
      "Requirement already satisfied: tensorboard in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: gradio in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (5.29.0)\n",
      "Collecting gradio\n",
      "  Downloading gradio-5.31.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: datasets[audio] in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.31.1)\n",
      "Requirement already satisfied: packaging in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (6.0.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.13.1)\n",
      "Requirement already satisfied: librosa in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.11.0)\n",
      "Requirement already satisfied: soxr>=0.4.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.5.0.post1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (3.11.18)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets[audio]) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets[audio]) (1.1.0)\n",
      "Requirement already satisfied: psutil in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from accelerate) (2.7.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from jiwer) (3.13.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (3.8)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (80.3.1)\n",
      "Requirement already satisfied: six>1.9 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.115.12)\n",
      "Requirement already satisfied: ffmpy in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.5.0)\n",
      "Collecting gradio-client==1.10.1 (from gradio)\n",
      "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: groovy~=0.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (11.2.1)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (2.11.4)\n",
      "Requirement already satisfied: pydub in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.11.8)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.15.3)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.34.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pandas->datasets[audio]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pandas->datasets[audio]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pandas->datasets[audio]) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (1.20.0)\n",
      "Requirement already satisfied: certifi in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets[audio]) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets[audio]) (2.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from soundfile>=0.12.1->datasets[audio]) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.22)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.5.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.8.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.1.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from numba>=0.51.0->librosa->datasets[audio]) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pooch>=1.1->librosa->datasets[audio]) (4.3.7)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from scikit-learn>=1.1.0->librosa->datasets[audio]) (3.6.0)\n",
      "Downloading transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m147.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Downloading gradio-5.31.0-py3-none-any.whl (54.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m151.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
      "Installing collected packages: gradio-client, transformers, gradio, accelerate\n",
      "\u001b[2K  Attempting uninstall: gradio-client\n",
      "\u001b[2K    Found existing installation: gradio_client 1.10.0\n",
      "\u001b[2K    Uninstalling gradio_client-1.10.0:\n",
      "\u001b[2K      Successfully uninstalled gradio_client-1.10.0\n",
      "\u001b[2K  Attempting uninstall: transformers\n",
      "\u001b[2K    Found existing installation: transformers 4.51.3\n",
      "\u001b[2K    Uninstalling transformers-4.51.3:38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.51.38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: gradio━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: gradio 5.29.0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling gradio-5.29.0:\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled gradio-5.29.0[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: accelerate━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [gradio]s]\n",
      "\u001b[2K    Found existing installation: accelerate 1.6.037m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [gradio]\n",
      "\u001b[2K    Uninstalling accelerate-1.6.0:━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [gradio]\n",
      "\u001b[2K      Successfully uninstalled accelerate-1.6.0;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [gradio]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [accelerate]\u001b[0m \u001b[32m3/4\u001b[0m [accelerate]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.7.0 gradio-5.31.0 gradio-client-1.10.1 transformers-4.52.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "!pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2364a5c2-048f-48c9-b87c-b41ec8505eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, TrainingArguments, Trainer\n",
    "from datasets import Dataset, Audio\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c7240-0a97-4be3-9e31-ffbe89e9bf33",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe211bd6-9b93-46f1-8203-edae9bf2311a",
   "metadata": {},
   "source": [
    "Using the guideline: https://huggingface.co/blog/fine-tune-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8707b470-b0be-48e2-9c32-cd566ab2a03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: ../creolese-audio-dataset/finetune_eligible/1a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/1b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/1c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/1d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/1e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/3a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/3b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/2.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/4a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/4b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/4c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/4d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/4e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5h.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5i.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5j.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5k.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5l.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5m.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6h.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6i.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6j.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6k.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6l.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6m.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6n.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6o.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6p.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/7.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8h.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8i.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8j.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8k.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/9a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/9b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/10a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/10b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/11a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/11b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/11c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/12.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/13.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/14.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/15a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/15b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/16a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/16b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/16c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/17a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/17b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/17c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/17d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18h.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18i.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18j.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18k.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18l.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18m.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18n.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18o.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18p.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18q.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18r.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18s.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18t.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18u.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18v.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18w.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18x.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18y.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18z.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18aa.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18ab.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18ac.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18ad.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18ae.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19h.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19i.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19j.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19k.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19l.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19m.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19n.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19o.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19p.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19q.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19r.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19s.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19t.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19u.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19v.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19w.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19x.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19y.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19z.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19aa.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19ab.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19ac.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19ad.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/20.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/21.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/22a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/22b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/22c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/22d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/23a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/23b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/23c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/23d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/23e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/23f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/23g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/24a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/24b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/25a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/25b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/26a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/26b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/26c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/26d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/28a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/28b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/28c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/28d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/28e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/28f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/29a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/29b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/29c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/29d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/29e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/30a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/30b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/30c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/30d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/30e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/30f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/30g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/31a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/31b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/31c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/31d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/32a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/32b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/32c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/32d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/35a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/35b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/27.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/42a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/42b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/42c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/43a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/43b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/43c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44h.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44i.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44j.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44k.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44l.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/45a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/45b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/45c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/45d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46h.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46i.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46j.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46k.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46l.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46m.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46n.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46o.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46p.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46q.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46r.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46s.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/33.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/34.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/36.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./41.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./47.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/51.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./56.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./57.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./60.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./62.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./63.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./64.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./68.wav\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"../creolese-audio-dataset/finetune_eligible\"\n",
    "transcription_path = \"../creolese-audio-dataset/finetune_eligible/transcripts.json\"\n",
    "\n",
    "# Load transcripts JSON\n",
    "with open(transcription_path, 'r') as f:\n",
    "    transcripts = json.load(f)\n",
    "\n",
    "# Create a list of dicts pairing audio files and transcripts\n",
    "data = []\n",
    "for item in transcripts:\n",
    "    audio_file = os.path.join(audio_path, item['audio'])\n",
    "    if os.path.exists(audio_file):\n",
    "        print(f\"Found file: {audio_file}\")\n",
    "        data.append({'audio': audio_file, 'text': item['text']})\n",
    "    else:\n",
    "        print(f\"Missing file: {audio_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c52f6f43-c61a-400b-8a62-006bced68163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'text'],\n",
      "    num_rows: 239\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Cast the audio column to automatically load audio\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8123ecc8-2548-40b5-846a-2659d6165372",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a426221-c1b1-4b98-b757-bbda74bbba1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51866, 1280, padding_idx=50256)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1280, out_features=51866, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"  \n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large-v3\")\n",
    "processor = WhisperProcessor.from_pretrained(model_id, task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0588296f-f843-427d-8563-128b3e30418c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b18c7389ce4459bce972631f28b1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/239 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#explicitly pad the data\n",
    "def pad_input_features(features, target_length=3000):\n",
    "    padded = []\n",
    "    for f in features:\n",
    "        current_length = f.shape[-1]\n",
    "        if current_length < target_length:\n",
    "            pad_width = target_length - current_length\n",
    "            f = np.pad(f, ((0, 0), (0, pad_width)), mode=\"constant\", constant_values=0)\n",
    "        padded.append(f)\n",
    "    return padded\n",
    "\n",
    "def batch_prepare_dataset(examples):\n",
    "    audio_arrays = [audio[\"array\"] for audio in examples[\"audio\"]]\n",
    "    sampling_rates = [audio[\"sampling_rate\"] for audio in examples[\"audio\"]]\n",
    "\n",
    "    # Extract input features (mel spectrograms)\n",
    "    input_features = processor.feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=sampling_rates[0],\n",
    "        return_tensors=\"np\"\n",
    "    )[\"input_features\"]\n",
    "\n",
    "    # Pad each feature to length 3000\n",
    "    input_features = pad_input_features(input_features)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    input_features = torch.tensor(np.array(input_features))\n",
    "\n",
    "    # Process labels\n",
    "    labels = processor.tokenizer(\n",
    "        examples[\"text\"], return_tensors=\"pt\", padding=True\n",
    "    ).input_ids\n",
    "\n",
    "    return {\n",
    "        \"input_features\": input_features,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "# Process in batches\n",
    "prepared_dataset = dataset.map(\n",
    "    batch_prepare_dataset,\n",
    "    batched=True,\n",
    "    batch_size=1,  # Adjust based on memory\n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "040f7f73-249a-4643-ba3c-4dbe5d591394",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.task = \"transcribe\"\n",
    "model.generation_config.language = None \n",
    "model.generation_config.forced_decoder_ids = None\n",
    "\n",
    "#This is unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6ac12ad-bd54-41d9-87aa-9138dac8cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [ f[\"input_features\"] for f in features]\n",
    "        batch = self.processor.feature_extractor.pad({\"input_features\": input_features}, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "    \n",
    "        # Remove BOS if present\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all():\n",
    "            labels = labels[:, 1:]\n",
    "    \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b705f79-84ad-4aa1-8a1e-50f0eb4a5316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, update your batch_prepare_dataset function\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Handle input features separately with proper padding\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"].unsqueeze(0)} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Handle labels\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 for loss calculation\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # Remove BOS token if present\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22c2edae-d1f1-4d81-89eb-8b18da502c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "\n",
    "transform = jiwer.Compose([\n",
    "        jiwer.ToLowerCase(),\n",
    "        jiwer.RemovePunctuation(),\n",
    "        jiwer.Strip(),\n",
    "        jiwer.RemoveMultipleSpaces(),\n",
    "])\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = torch.argmax(torch.tensor(pred_logits), dim=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = jiwer.wer(label_str, pred_str)\n",
    "    mer = jiwer.mer(label_str, pred_str)\n",
    "    cer = jiwer.cer(label_str, pred_str)\n",
    "    return {\"wer\": wer, \"mer\": mer, \"cer\": cer}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13fb7c94-cd4f-469d-b3be-9ce3d4f79f3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-large-v3-creolese-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    max_steps=50,  \n",
    "    gradient_checkpointing=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    do_eval= True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=2,\n",
    "    logging_steps=5,\n",
    "    report_to=[\"tensorboard\"],  # or [\"none\"]\n",
    "    push_to_hub=False,\n",
    "    eval_steps=5,            # Every 50 steps\n",
    "    eval_strategy=\"steps\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5b7ba82-333d-41cf-b5be-cfd3c124316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=prepared_dataset,\n",
    "    eval_dataset=prepared_dataset,  # or add eval split if available\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor.feature_extractor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f15a5426-2a28-45a9-a015-71d61cd680b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/50 6:46:05 < 8:07:18, 0.00 it/s, Epoch 0.21/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Mer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.515300</td>\n",
       "      <td>2.106143</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>0.999401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.083200</td>\n",
       "      <td>1.820553</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>0.999360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.606200</td>\n",
       "      <td>1.636009</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.999442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.532400</td>\n",
       "      <td>1.517083</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>0.999513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/30 1:02:27 < 38:09, 0.00 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# trainer.train()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:2622\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2620\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2621\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2622\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2633\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:3095\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3093\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3094\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3095\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3096\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3098\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:3044\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3044\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3045\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3047\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/trainer_seq2seq.py:191\u001b[39m, in \u001b[36mSeq2SeqTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28mself\u001b[39m.gather_function = \u001b[38;5;28mself\u001b[39m.accelerator.gather\n\u001b[32m    190\u001b[39m \u001b[38;5;28mself\u001b[39m._gen_kwargs = gen_kwargs\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:4173\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4170\u001b[39m start_time = time.time()\n\u001b[32m   4172\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4173\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4174\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4176\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4177\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4181\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4183\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:4368\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4365\u001b[39m         batch_size = observed_batch_size\n\u001b[32m   4367\u001b[39m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4368\u001b[39m losses, logits, labels = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4369\u001b[39m main_input_name = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mmain_input_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4370\u001b[39m inputs_decode = (\n\u001b[32m   4371\u001b[39m     \u001b[38;5;28mself\u001b[39m._prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args.include_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4372\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/trainer_seq2seq.py:327\u001b[39m, in \u001b[36mSeq2SeqTrainer.prediction_step\u001b[39m\u001b[34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m summon_full_params_context = (\n\u001b[32m    321\u001b[39m     FullyShardedDataParallel.summon_full_params(\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, FullyShardedDataParallel)\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext()\n\u001b[32m    324\u001b[39m )\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m summon_full_params_context:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     generated_tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001b[39;00m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.generation_config._from_model_config:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:774\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate\u001b[39m\u001b[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, **kwargs)\u001b[39m\n\u001b[32m    765\u001b[39m             proc.set_begin_index(decoder_input_ids.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m    767\u001b[39m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[32m    768\u001b[39m (\n\u001b[32m    769\u001b[39m     seek_sequences,\n\u001b[32m    770\u001b[39m     seek_outputs,\n\u001b[32m    771\u001b[39m     should_skip,\n\u001b[32m    772\u001b[39m     do_condition_on_prev_tokens,\n\u001b[32m    773\u001b[39m     model_output_type,\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:950\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate_with_fallback\u001b[39m\u001b[34m(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[39m\n\u001b[32m    945\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    946\u001b[39m         generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m] = F.pad(\n\u001b[32m    947\u001b[39m             generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m], (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, batch_size - cur_bsz), value=\u001b[32m0\u001b[39m\n\u001b[32m    948\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m950\u001b[39m seek_outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    962\u001b[39m model_output_type = \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[32m    964\u001b[39m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/generation/utils.py:2412\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2408\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`attention_mask` passed to `generate` must be 2D.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[32m   2411\u001b[39m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2412\u001b[39m     model_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2413\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[32m   2414\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[32m   2417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/generation/utils.py:854\u001b[39m, in \u001b[36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[39m\u001b[34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[39m\n\u001b[32m    852\u001b[39m encoder_kwargs[\u001b[33m\"\u001b[39m\u001b[33mreturn_dict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    853\u001b[39m encoder_kwargs[model_input_name] = inputs_tensor\n\u001b[32m--> \u001b[39m\u001b[32m854\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m]: ModelOutput = \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    856\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:931\u001b[39m, in \u001b[36mWhisperEncoder.forward\u001b[39m\u001b[34m(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    923\u001b[39m         layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    924\u001b[39m             encoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    925\u001b[39m             hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    928\u001b[39m             output_attentions,\n\u001b[32m    929\u001b[39m         )\n\u001b[32m    930\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m         layer_outputs = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    938\u001b[39m     hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:615\u001b[39m, in \u001b[36mWhisperEncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[39m\n\u001b[32m    613\u001b[39m residual = hidden_states\n\u001b[32m    614\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.self_attn_layer_norm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m hidden_states, attn_weights, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n\u001b[32m    622\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:542\u001b[39m, in \u001b[36mWhisperSdpaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions, cache_position)\u001b[39m\n\u001b[32m    538\u001b[39m is_causal = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_causal \u001b[38;5;129;01mand\u001b[39;00m causal_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    540\u001b[39m \u001b[38;5;66;03m# NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\u001b[39;00m\n\u001b[32m    541\u001b[39m \u001b[38;5;66;03m# but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attn_output.size() != (bsz, \u001b[38;5;28mself\u001b[39m.num_heads, tgt_len, \u001b[38;5;28mself\u001b[39m.head_dim):\n\u001b[32m    552\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    553\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`attn_output` should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m.num_heads,\u001b[38;5;250m \u001b[39mtgt_len,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m.head_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but is\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    554\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_output.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    555\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8047c3da-75b0-4c1e-947d-a1c7efd26e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
