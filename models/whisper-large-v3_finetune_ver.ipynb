{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32de85da-7995-424c-b8e7-35d498d5983c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (4.51.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: accelerate in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (1.6.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: evaluate in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (0.4.3)\n",
      "Requirement already satisfied: jiwer in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (3.1.0)\n",
      "Requirement already satisfied: tensorboard in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: gradio in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (5.29.0)\n",
      "Collecting gradio\n",
      "  Downloading gradio-5.31.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: datasets[audio] in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.31.1)\n",
      "Requirement already satisfied: packaging in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (6.0.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.13.1)\n",
      "Requirement already satisfied: librosa in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.11.0)\n",
      "Requirement already satisfied: soxr>=0.4.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.5.0.post1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (3.11.18)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets[audio]) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets[audio]) (1.1.0)\n",
      "Requirement already satisfied: psutil in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from accelerate) (2.7.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from jiwer) (3.13.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (3.8)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (80.3.1)\n",
      "Requirement already satisfied: six>1.9 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.115.12)\n",
      "Requirement already satisfied: ffmpy in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.5.0)\n",
      "Collecting gradio-client==1.10.1 (from gradio)\n",
      "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: groovy~=0.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (11.2.1)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (2.11.4)\n",
      "Requirement already satisfied: pydub in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.11.8)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.15.3)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.34.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pandas->datasets[audio]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pandas->datasets[audio]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pandas->datasets[audio]) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (1.20.0)\n",
      "Requirement already satisfied: certifi in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets[audio]) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets[audio]) (2.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from soundfile>=0.12.1->datasets[audio]) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.22)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.5.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.8.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.1.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from numba>=0.51.0->librosa->datasets[audio]) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pooch>=1.1->librosa->datasets[audio]) (4.3.7)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from scikit-learn>=1.1.0->librosa->datasets[audio]) (3.6.0)\n",
      "Downloading transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m147.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Downloading gradio-5.31.0-py3-none-any.whl (54.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m151.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
      "Installing collected packages: gradio-client, transformers, gradio, accelerate\n",
      "\u001b[2K  Attempting uninstall: gradio-client\n",
      "\u001b[2K    Found existing installation: gradio_client 1.10.0\n",
      "\u001b[2K    Uninstalling gradio_client-1.10.0:\n",
      "\u001b[2K      Successfully uninstalled gradio_client-1.10.0\n",
      "\u001b[2K  Attempting uninstall: transformers\n",
      "\u001b[2K    Found existing installation: transformers 4.51.3\n",
      "\u001b[2K    Uninstalling transformers-4.51.3:38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.51.38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: gradio━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: gradio 5.29.0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling gradio-5.29.0:\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled gradio-5.29.0[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: accelerate━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [gradio]s]\n",
      "\u001b[2K    Found existing installation: accelerate 1.6.037m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [gradio]\n",
      "\u001b[2K    Uninstalling accelerate-1.6.0:━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [gradio]\n",
      "\u001b[2K      Successfully uninstalled accelerate-1.6.0;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [gradio]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [accelerate]\u001b[0m \u001b[32m3/4\u001b[0m [accelerate]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.7.0 gradio-5.31.0 gradio-client-1.10.1 transformers-4.52.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "!pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2364a5c2-048f-48c9-b87c-b41ec8505eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, TrainingArguments, Trainer\n",
    "from datasets import Dataset, Audio\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c7240-0a97-4be3-9e31-ffbe89e9bf33",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe211bd6-9b93-46f1-8203-edae9bf2311a",
   "metadata": {},
   "source": [
    "Using the guideline: https://huggingface.co/blog/fine-tune-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8707b470-b0be-48e2-9c32-cd566ab2a03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/2.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/7.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/12.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/13.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/14.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/20.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/21.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/27.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/33.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/34.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/36.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./41.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./47.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./51.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./56.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./57.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./60.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./62.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./63.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./64.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./68.wav\n",
      "Missing file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./.wav\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"./creolese-audio-dataset/Audio Files/finetune_eligible\"\n",
    "transcription_path = \"./creolese-audio-dataset/Audio Files/finetune_eligible/transcripts.json\"\n",
    "\n",
    "# Load transcripts JSON\n",
    "with open(transcription_path, 'r') as f:\n",
    "    transcripts = json.load(f)\n",
    "\n",
    "# Create a list of dicts pairing audio files and transcripts\n",
    "data = []\n",
    "for item in transcripts:\n",
    "    audio_file = os.path.join(audio_path, item['audio'])\n",
    "    if os.path.exists(audio_file):\n",
    "        print(f\"Found file: {audio_file}\")\n",
    "        data.append({'audio': audio_file, 'text': item['text']})\n",
    "    else:\n",
    "        print(f\"Missing file: {audio_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c52f6f43-c61a-400b-8a62-006bced68163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'text'],\n",
      "    num_rows: 21\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Cast the audio column to automatically load audio\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8123ecc8-2548-40b5-846a-2659d6165372",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a426221-c1b1-4b98-b757-bbda74bbba1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51866, 1280, padding_idx=50256)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1280, out_features=51866, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"  \n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large-v3\")\n",
    "processor = WhisperProcessor.from_pretrained(model_id, task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef341482-1bb9-419c-be42-96bf6a8870f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76e1361d6374c04841b098bcf5db3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT USE THIS\n",
    "\n",
    "def batch_prepare_dataset(examples):\n",
    "    audio_arrays = [audio[\"array\"] for audio in examples[\"audio\"]]\n",
    "    sampling_rates = [audio[\"sampling_rate\"] for audio in examples[\"audio\"]]\n",
    "\n",
    "    # Process all examples in a batch\n",
    "    inputs = processor(\n",
    "        audio_arrays, \n",
    "        sampling_rate=sampling_rates[0],  # Assuming all are same rate\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Process all texts in batch\n",
    "    labels = processor.tokenizer(examples[\"text\"], return_tensors=\"pt\", padding=True).input_ids\n",
    "\n",
    "    return {\n",
    "        \"input_features\": inputs[\"input_features\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Process in batches\n",
    "prepared_dataset = dataset.map(\n",
    "    batch_prepare_dataset,\n",
    "    batched=True,\n",
    "    batch_size=1,  # Adjust based on memory\n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0588296f-f843-427d-8563-128b3e30418c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe7c1315d3346b0a7bbb93970253da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_input_features(features, target_length=3000):\n",
    "    padded = []\n",
    "    for f in features:\n",
    "        current_length = f.shape[-1]\n",
    "        if current_length < target_length:\n",
    "            pad_width = target_length - current_length\n",
    "            f = np.pad(f, ((0, 0), (0, pad_width)), mode=\"constant\", constant_values=0)\n",
    "        padded.append(f)\n",
    "    return padded\n",
    "\n",
    "def batch_prepare_dataset(examples):\n",
    "    audio_arrays = [audio[\"array\"] for audio in examples[\"audio\"]]\n",
    "    sampling_rates = [audio[\"sampling_rate\"] for audio in examples[\"audio\"]]\n",
    "\n",
    "    # Extract input features (mel spectrograms)\n",
    "    input_features = processor.feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=sampling_rates[0],\n",
    "        return_tensors=\"np\"\n",
    "    )[\"input_features\"]\n",
    "\n",
    "    # Pad each feature to length 3000\n",
    "    input_features = pad_input_features(input_features)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    input_features = torch.tensor(np.array(input_features))\n",
    "\n",
    "    # Process labels\n",
    "    labels = processor.tokenizer(\n",
    "        examples[\"text\"], return_tensors=\"pt\", padding=True\n",
    "    ).input_ids\n",
    "\n",
    "    return {\n",
    "        \"input_features\": input_features,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "# Process in batches\n",
    "prepared_dataset = dataset.map(\n",
    "    batch_prepare_dataset,\n",
    "    batched=True,\n",
    "    batch_size=1,  # Adjust based on memory\n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "040f7f73-249a-4643-ba3c-4dbe5d591394",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.task = \"transcribe\"\n",
    "model.generation_config.language = None \n",
    "model.generation_config.forced_decoder_ids = None\n",
    "\n",
    "#This is unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6ac12ad-bd54-41d9-87aa-9138dac8cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [ f[\"input_features\"] for f in features]\n",
    "        batch = self.processor.feature_extractor.pad({\"input_features\": input_features}, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "    \n",
    "        # Remove BOS if present\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all():\n",
    "            labels = labels[:, 1:]\n",
    "    \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b705f79-84ad-4aa1-8a1e-50f0eb4a5316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, update your batch_prepare_dataset function\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Handle input features separately with proper padding\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"].unsqueeze(0)} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Handle labels\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 for loss calculation\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # Remove BOS token if present\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22c2edae-d1f1-4d81-89eb-8b18da502c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "\n",
    "transform = jiwer.Compose([\n",
    "        jiwer.ToLowerCase(),\n",
    "        jiwer.RemovePunctuation(),\n",
    "        jiwer.Strip(),\n",
    "        jiwer.RemoveMultipleSpaces(),\n",
    "])\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = jiwer.wer(\n",
    "        ground_truth=label_str,\n",
    "        hypothesis=pred_str,\n",
    "        truth_transform=transform,\n",
    "        hypothesis_transform=transform\n",
    "    ) \n",
    "    mer = jiwer.mer(\n",
    "        ground_truth=label_str,\n",
    "        hypothesis=pred_str,\n",
    "        truth_transform=transform,\n",
    "        hypothesis_transform=transform\n",
    "    ) \n",
    "    cer = jiwer.cer(\n",
    "        ground_truth=label_str,\n",
    "        hypothesis=pred_str,\n",
    "        truth_transform=transform,\n",
    "        hypothesis_transform=transform\n",
    "    ) \n",
    "    return {\"wer\": wer, \"cer\": cer, \"mer\": mer}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13fb7c94-cd4f-469d-b3be-9ce3d4f79f3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-large-v3-creolese-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    max_steps=50,  \n",
    "    gradient_checkpointing=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    do_eval= True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=2,\n",
    "    logging_steps=5,\n",
    "    report_to=[\"tensorboard\"],  # or [\"none\"]\n",
    "    push_to_hub=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5b7ba82-333d-41cf-b5be-cfd3c124316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4886/2944772423.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=prepared_dataset,\n",
    "    eval_dataset=prepared_dataset,  # or add eval split if available\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f15a5426-2a28-45a9-a015-71d61cd680b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 54:19, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.333900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.234100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.650200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.630900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.530300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.422300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.355300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.385900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.991521954536438, metrics={'train_runtime': 3319.2355, 'train_samples_per_second': 0.03, 'train_steps_per_second': 0.015, 'total_flos': 3.2615983742976e+17, 'train_loss': 0.991521954536438, 'epoch': 4.571428571428571})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8047c3da-75b0-4c1e-947d-a1c7efd26e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
