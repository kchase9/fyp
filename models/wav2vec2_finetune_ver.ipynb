{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d818058-a076-4917-bc1f-ef3c0218180a",
   "metadata": {},
   "source": [
    "# This is the script in which we train Wav2Vec2 on the Creolese Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e248cc4b-0951-4e34-834f-2136f7e04beb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_xet in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (1.1.2)\n",
      "Requirement already satisfied: transformers[torch] in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (4.52.3)\n",
      "Requirement already satisfied: filelock in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from transformers[torch]) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from transformers[torch]) (0.31.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from transformers[torch]) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from transformers[torch]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch<2.7,>=2.1 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from transformers[torch]) (2.6.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from transformers[torch]) (1.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.13.2)\n",
      "Requirement already satisfied: networkx in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from sympy==1.13.1->torch<2.7,>=2.1->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: psutil in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from jinja2->torch<2.7,>=2.1->transformers[torch]) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from requests->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from requests->transformers[torch]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kris/Documents/fyp/fyp_env/lib/python3.11/site-packages (from requests->transformers[torch]) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "#!pip install datasets transformers torchaudio jiwer librosa soundfile\n",
    "!pip install transformers[torch] hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ec5764-37de-4b0a-9e78-f3f8acff2892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset, Audio\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87c9d18f-94bf-4c1f-864b-d4df44354a61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: ../creolese-audio-dataset/finetune_eligible/1a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/1b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/1c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/1d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/1e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/3a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/3b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/2.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/4a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/4b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/4c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/4d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/4e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5h.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5i.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5j.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5k.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5l.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/5m.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6h.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6i.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6j.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6k.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6l.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6m.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6n.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6o.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/6p.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/7.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8h.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8i.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8j.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/8k.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/9a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/9b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/10a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/10b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/11a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/11b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/11c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/12.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/13.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/14.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/15a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/15b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/16a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/16b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/16c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/17a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/17b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/17c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/17d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18h.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18i.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18j.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18k.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18l.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18m.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18n.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18o.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18p.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18q.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18r.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18s.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18t.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18u.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18v.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18w.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18x.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18y.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18z.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18aa.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18ab.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18ac.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18ad.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18ae.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/18f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19h.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19i.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19j.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19k.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19l.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19m.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19n.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19o.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19p.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19q.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19r.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19s.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19t.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19u.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19v.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19w.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19x.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19y.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19z.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19aa.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19ab.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19ac.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/19ad.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/20.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/21.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/22a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/22b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/22c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/22d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/23a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/23b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/23c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/23d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/23e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/23f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/23g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/24a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/24b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/25a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/25b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/26a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/26b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/26c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/26d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/28a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/28b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/28c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/28d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/28e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/28f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/29a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/29b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/29c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/29d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/29e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/30a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/30b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/30c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/30d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/30e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/30f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/30g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/31a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/31b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/31c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/31d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/32a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/32b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/32c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/32d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/35a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/35b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/27.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/42a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/42b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/42c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/43a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/43b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/43c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44h.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44i.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44j.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44k.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/44l.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/45a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/45b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/45c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/45d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46a.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46b.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46c.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46d.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46e.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46f.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46g.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46h.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46i.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46j.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46k.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46l.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46m.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46n.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46o.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46p.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46q.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46r.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/46s.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/33.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/34.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/36.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./41.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./47.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/51.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./56.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./57.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./60.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./62.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./63.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./64.wav\n",
      "Found file: ../creolese-audio-dataset/finetune_eligible/./68.wav\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"../creolese-audio-dataset/finetune_eligible\"\n",
    "transcription_path = \"../creolese-audio-dataset/finetune_eligible/transcripts.json\"\n",
    "\n",
    "# Load transcripts JSON\n",
    "with open(transcription_path, 'r') as f:\n",
    "    transcripts = json.load(f)\n",
    "\n",
    "# Create a list of dicts pairing audio files and transcripts\n",
    "data = []\n",
    "for item in transcripts:\n",
    "    audio_file = os.path.join(audio_path, item['audio'])\n",
    "    if os.path.exists(audio_file):\n",
    "        print(f\"Found file: {audio_file}\")\n",
    "        data.append({'audio': audio_file, 'text': item['text']})\n",
    "    else:\n",
    "        print(f\"Missing file: {audio_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9939099-8602-4e98-91d4-cb564e3dd9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'text'],\n",
      "    num_rows: 239\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Cast the audio column to automatically load audio\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26fcb2b8-ae82-4e3f-82b5-558254582694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into 80% training, 20% evaluation\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ee6c0d-5c41-4ed0-bdc1-c0e52aba6830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token ID: 40\n",
      "Vocab size: 53\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2Processor\n",
    "\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1,\n",
    "    sampling_rate=16000,\n",
    "    padding_value=0.0,\n",
    "    do_normalize=True,\n",
    "    return_attention_mask=False\n",
    ")\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"../tokenizer_files/vocab (Copy).json\", unk_token=\"<unk>\", pad_token=\"<pad>\", word_delimiter_token=\"|\")\n",
    "print(f\"Pad token ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n",
    "processor = Wav2Vec2Processor(feature_extractor, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb86bb6e-7d11-4c79-b593-4c18c3531777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4851fc1318140348e6d0ba7528170d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/191 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a77e626683542c9b3a61addb38eb364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/48 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataset(batch, processor):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # Get input values from audio\n",
    "    input_values = processor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_values[0]\n",
    "\n",
    "    # Get labels from text\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "    \n",
    "\n",
    "    # Return proper format for CTC\n",
    "    return {\n",
    "        \"input_values\": input_values,\n",
    "        \"labels\": batch[\"labels\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply preprocessing to both splits\n",
    "train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, num_proc=4, fn_kwargs={\"processor\": processor})\n",
    "eval_dataset = eval_dataset.map(prepare_dataset, remove_columns=eval_dataset.column_names, num_proc=4, fn_kwargs={\"processor\": processor})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f815e459-77f2-4970-bfa1-dff9350cb6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized because the shapes did not match:\n",
      "- lm_head.bias: found shape torch.Size([32]) in the checkpoint and torch.Size([53]) in the model instantiated\n",
      "- lm_head.weight: found shape torch.Size([32, 768]) in the checkpoint and torch.Size([53, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    # \"facebook/wav2vec2-large-960h\",\n",
    "    \"facebook/wav2vec2-base-960h\",\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(tokenizer),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.wav2vec2.feature_extractor._freeze_parameters()\n",
    "torch.nn.init.normal_(model.wav2vec2.masked_spec_embed, mean=0.0, std=0.02)\n",
    "torch.nn.init.constant_(model.lm_head.bias, -1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed58c0-4cc1-4663-a45c-2464dbee47f0",
   "metadata": {},
   "source": [
    "# This is a custom attempt of a CTC Data Collator\n",
    "Maybe you can include a description of what this is, what the issue is with the other one and why this is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4336151-5eca-4e83-9d28-c1338f9a2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Any\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Proper data collator for Wav2Vec2 CTC training\n",
    "    This fixes the zero loss issue\n",
    "    \"\"\"\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Union[int, None] = None\n",
    "    max_length_labels: Union[int, None] = None\n",
    "    pad_to_multiple_of: Union[int, None] = None\n",
    "    pad_to_multiple_of_labels: Union[int, None] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Separate inputs and labels\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Pad input features\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Pad labels\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length_labels,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Replace padding with -100 to ignore in loss\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1470fb65-bb60-42e8-b7ac-f98f4ed742f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97225c1f-edae-4b01-a080-fd6156671abd",
   "metadata": {},
   "source": [
    "# Let's start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e419c8-eac7-4eb4-9b0f-60f16ca340d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate>=0.26.0 transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89cc2afa-4002-4fba-8bb0-36f7a6af925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jiwer\n",
    "# import torch\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     pred_logits = pred.predictions\n",
    "#     pred_ids = torch.argmax(torch.tensor(pred_logits), dim=-1)\n",
    "#     pred_str = processor.batch_decode(pred_ids)\n",
    "#     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "#     wer = jiwer.wer(label_str, pred_str)\n",
    "#     mer = jiwer.mer(label_str, pred_str)\n",
    "#     cer = jiwer.cer(label_str, pred_str)\n",
    "#     return {\"wer\": wer, \"mer\": mer, \"cer\": cer}\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = torch.argmax(torch.tensor(pred_logits), dim=-1)\n",
    "\n",
    "    # Clean predictions\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Fix labels - replace -100 with pad token\n",
    "    label_ids = pred.label_ids.copy()\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Debug prints\n",
    "    print(f\"Sample pred: '{pred_str[0]}'\")\n",
    "    print(f\"Sample label: '{label_str[0]}'\")\n",
    "\n",
    "    try:\n",
    "        wer = jiwer.wer(label_str, pred_str)\n",
    "        mer = jiwer.mer(label_str, pred_str)\n",
    "        cer = jiwer.cer(label_str, pred_str)\n",
    "        return {\"wer\": wer, \"mer\": mer, \"cer\": cer}\n",
    "    except:\n",
    "        return {\"fallback wer\": 1.0, \"fallback mer\": 1.0, \"fallback cer\": 1.0, }  # Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "815a0e11-8ffc-4046-9684-81abdda296d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../training_outputs/wav2vec2_training_output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    dataloader_num_workers=0,\n",
    "    logging_steps=5,\n",
    "    save_steps=2,\n",
    "    learning_rate=1e-5, #lower learning rate\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    eval_steps=5,            # Every 50 steps\n",
    "    eval_strategy=\"steps\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "987bf2f4-e10b-4ee4-805f-4a41498411d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=processor.feature_extractor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "160af185-c648-48fd-90ab-7cb2773f0af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing one training step...\n",
      "Test loss: 15.605981826782227\n"
     ]
    }
   ],
   "source": [
    "# Test one batch first\n",
    "print(\"Testing one training step...\")\n",
    "batch = [train_dataset[0]]\n",
    "collated = data_collator(batch)\n",
    "outputs = model(**collated)\n",
    "print(f\"Test loss: {outputs.loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4016ccd-3b4f-4c57-a03d-622e14cc6cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 16/240 09:20 < 2:29:29, 0.02 it/s, Epoch 0.31/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Fallback wer</th>\n",
       "      <th>Fallback mer</th>\n",
       "      <th>Fallback cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>16.903500</td>\n",
       "      <td>23.668852</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>14.836100</td>\n",
       "      <td>22.626188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample pred: '(-hn)oo:hn:oung:)ooehooouam(p:(:oomhnoua-pdoohnrkwouoo)s:dkakwd:ruhnuahnur:ouoo):soodkakwou:ee,:nounghn):(-uo:r-:v:(-mpou:oo):ohnmaouehoo-kwnkw-poo:ou-rooovdkw:r:rngm:ochpoomoukwoo:ooou:ngp:uuo-m(ksya-m(pypmoo(:a-moo)puou)soodkahnvyee)ng:ya:(poo:ouak)(hn:hn:kw:oogeeeoukwouoo:ooouu(gmsh:oung:poo)oodkwooeoo:s:dahnou:-oo:oueeedp(oueeou(moouungoo-ng):m(p:yo)pooo)oo-:a:ohn):rmoovd:pouaoo:ao-oomp-n?-mp?:ngmouuup)-rooouung)apy-)avd:kwmehp!ouhndroo:oupookwjryooeh:-ouoouou)pmeeoo:ookwjeeao)ooroaooodo-m(rd)oouuthnnghn:)(:p:mpchooksyang:?-:(:oueeey,)kw'\n",
      "Sample label: 'riiyeerz intu yuujiidatairiliidooniivin riliilaik koodinglaik da rait. i wono om,aiwono fookos moor on art koz das somtingdatairiliilaik, koz, yu noo,aim a kriiyeetiv porson. nd om,aidoon riliihav a lot ov art stof, om, biikaazai,aikaainda dischroyd dem a wail bak, om. ozaidid vizuwol arts and maitiichor sokt. ndaiendid op heetin art for laik a gud'\n",
      "Sample pred: '(oohnoo:ou)oopam(po:oomhnapoo)s:dkkwd:ruhnuahnur:oo):sdee,:nghn:kwhnar:(-oo):oankw-p:ouoodkwoo:m:ochpoo:ooom(ksyo-m(poo:ao-oouou)sdkzng:a:(p:ak)(:hnooeouooo(msh:oopookwooeoo)soodkoooueeed(eeoomoong-(p:ooo:drmoodoua?-m?oo:ngmoupoongmap)avdkwmehpoudroopooeh:-uoupmeeooeeoooaoodoom(rd)oohnng:)(pooyaoo:oo:?-(:oueeey,)kw'\n",
      "Sample label: 'riiyeerz intu yuujiidatairiliidooniivin riliilaik koodinglaik da rait. i wono om,aiwono fookos moor on art koz das somtingdatairiliilaik, koz, yu noo,aim a kriiyeetiv porson. nd om,aidoon riliihav a lot ov art stof, om, biikaazai,aikaainda dischroyd dem a wail bak, om. ozaidid vizuwol arts and maitiichor sokt. ndaiendid op heetin art for laik a gud'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# trainer.train(resume_from_checkpoint=True)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m trainer.evaluate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:2622\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2620\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2621\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2622\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2633\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:3095\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3093\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3094\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3095\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3096\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3098\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:3044\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3044\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3045\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3047\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:4173\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4170\u001b[39m start_time = time.time()\n\u001b[32m   4172\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4173\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4174\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4176\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4177\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4181\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4183\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:4368\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4365\u001b[39m         batch_size = observed_batch_size\n\u001b[32m   4367\u001b[39m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4368\u001b[39m losses, logits, labels = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4369\u001b[39m main_input_name = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mmain_input_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4370\u001b[39m inputs_decode = (\n\u001b[32m   4371\u001b[39m     \u001b[38;5;28mself\u001b[39m._prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args.include_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4372\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:4584\u001b[39m, in \u001b[36mTrainer.prediction_step\u001b[39m\u001b[34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[39m\n\u001b[32m   4582\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[32m   4583\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4584\u001b[39m         loss, outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   4585\u001b[39m     loss = loss.detach().mean()\n\u001b[32m   4587\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:3810\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3808\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3809\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3810\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3811\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3812\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:2128\u001b[39m, in \u001b[36mWav2Vec2ForCTC.forward\u001b[39m\u001b[34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[39m\n\u001b[32m   2125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m labels.max() >= \u001b[38;5;28mself\u001b[39m.config.vocab_size:\n\u001b[32m   2126\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLabel values must be <= vocab_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.vocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2128\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwav2vec2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2129\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2131\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2132\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2134\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2136\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   2137\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1728\u001b[39m, in \u001b[36mWav2Vec2Model.forward\u001b[39m\u001b[34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1723\u001b[39m hidden_states, extract_features = \u001b[38;5;28mself\u001b[39m.feature_projection(extract_features)\n\u001b[32m   1724\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m._mask_hidden_states(\n\u001b[32m   1725\u001b[39m     hidden_states, mask_time_indices=mask_time_indices, attention_mask=attention_mask\n\u001b[32m   1726\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1728\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1731\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1732\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1734\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1736\u001b[39m hidden_states = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.adapter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1029\u001b[39m, in \u001b[36mWav2Vec2Encoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1022\u001b[39m         layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1023\u001b[39m             layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m   1024\u001b[39m             hidden_states,\n\u001b[32m   1025\u001b[39m             attention_mask,\n\u001b[32m   1026\u001b[39m             output_attentions,\n\u001b[32m   1027\u001b[39m         )\n\u001b[32m   1028\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1029\u001b[39m         layer_outputs = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1032\u001b[39m     hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1034\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m skip_the_layer:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:911\u001b[39m, in \u001b[36mWav2Vec2EncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions)\u001b[39m\n\u001b[32m    908\u001b[39m hidden_states = attn_residual + hidden_states\n\u001b[32m    910\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m911\u001b[39m hidden_states = hidden_states + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    912\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.final_layer_norm(hidden_states)\n\u001b[32m    914\u001b[39m outputs = (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:882\u001b[39m, in \u001b[36mWav2Vec2FeedForward.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    879\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.intermediate_act_fn(hidden_states)\n\u001b[32m    880\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.intermediate_dropout(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.output_dropout(hidden_states)\n\u001b[32m    884\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_env/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac52af6-288b-415a-b443-20652d50ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug what the model is actually predicting\n",
    "test_sample = eval_dataset[0]\n",
    "inputs = processor(test_sample[\"input_values\"], return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "print(f\"Raw predicted IDs: {predicted_ids}\")\n",
    "print(f\"Predicted tokens: {[processor.tokenizer.decode([id]) for id in predicted_ids[0]]}\")\n",
    "print(f\"Decoded text: '{processor.batch_decode(predicted_ids)[0]}'\")\n",
    "print(f\"Expected: '{processor.tokenizer.decode(test_sample['labels'])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b60d5d-1a67-4062-9c29-ffa67a3d6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./wav2vec2-creolese-finetuned\")\n",
    "processor.save_pretrained(\"./wav2vec2-creolese-finetuned\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
