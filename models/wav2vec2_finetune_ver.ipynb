{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d818058-a076-4917-bc1f-ef3c0218180a",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e248cc4b-0951-4e34-834f-2136f7e04beb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# pip install datasets transformers torchaudio jiwer librosa soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ec5764-37de-4b0a-9e78-f3f8acff2892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset, Audio\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c9d18f-94bf-4c1f-864b-d4df44354a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/2.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/7.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/12.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/13.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/14.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/20.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/21.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/27.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/33.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/34.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/36.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./41.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./47.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./51.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./56.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./57.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./60.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./62.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./63.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./64.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./68.wav\n",
      "Missing file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./.wav\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"./creolese-audio-dataset/Audio Files/finetune_eligible\"\n",
    "transcription_path = \"./creolese-audio-dataset/Audio Files/finetune_eligible/transcripts.json\"\n",
    "\n",
    "# Load transcripts JSON\n",
    "with open(transcription_path, 'r') as f:\n",
    "    transcripts = json.load(f)\n",
    "\n",
    "# Create a list of dicts pairing audio files and transcripts\n",
    "data = []\n",
    "for item in transcripts:\n",
    "    audio_file = os.path.join(audio_path, item['audio'])\n",
    "    if os.path.exists(audio_file):\n",
    "        print(f\"Found file: {audio_file}\")\n",
    "        data.append({'audio': audio_file, 'text': item['text']})\n",
    "    else:\n",
    "        print(f\"Missing file: {audio_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9939099-8602-4e98-91d4-cb564e3dd9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'text'],\n",
      "    num_rows: 21\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Cast the audio column to automatically load audio\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ed8551-ccd1-4636-a0d2-e4633fb78332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "# Load processor (tokenizer + feature extractor)\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "# SWITCHING TO CUSTOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ee6c0d-5c41-4ed0-bdc1-c0e52aba6830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2Processor\n",
    "\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1,\n",
    "    sampling_rate=16000,\n",
    "    padding_value=0.0,\n",
    "    do_normalize=True,\n",
    "    return_attention_mask=False\n",
    ")\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./tokenizer_files/vocab.json\", unk_token=\"<unk>\", pad_token=\"<pad>\", word_delimiter_token=\"|\")\n",
    "processor = Wav2Vec2Processor(feature_extractor, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb86bb6e-7d11-4c79-b593-4c18c3531777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1da7977d674d6ab91e98dd329c3950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataset(batch, processor):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # Get input values from audio\n",
    "    input_values = processor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_values[0]\n",
    "\n",
    "    # Get labels from text\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "    \n",
    "\n",
    "    # Return proper format for CTC\n",
    "    return {\n",
    "        \"input_values\": input_values,\n",
    "        \"labels\": batch[\"labels\"]\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names, num_proc=4, fn_kwargs={\"processor\": processor})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f815e459-77f2-4970-bfa1-dff9350cb6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized because the shapes did not match:\n",
      "- lm_head.weight: found shape torch.Size([32, 1024]) in the checkpoint and torch.Size([53, 1024]) in the model instantiated\n",
      "- lm_head.bias: found shape torch.Size([32]) in the checkpoint and torch.Size([53]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-960h\",\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=53,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed58c0-4cc1-4663-a45c-2464dbee47f0",
   "metadata": {},
   "source": [
    "# This is a custom attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4336151-5eca-4e83-9d28-c1338f9a2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Any\n",
    "\n",
    "@dataclass\n",
    "class SimpleCTCDataCollator:\n",
    "        processor: Any\n",
    "\n",
    "        def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "            # Get the input_values from each feature\n",
    "            input_values = [feature[\"input_values\"].squeeze(0) if isinstance(feature[\"input_values\"], torch.Tensor) else torch.tensor(feature[\"input_values\"]) for feature in features]\n",
    "\n",
    "            # Determine max length for padding\n",
    "            max_length = max(len(x) for x in input_values)\n",
    "\n",
    "            # Pad the input_values manually\n",
    "            padded_input_values = []\n",
    "            attention_mask = []\n",
    "\n",
    "            for val in input_values:\n",
    "                # Create attention mask (1 for real values, 0 for padding)\n",
    "                length = len(val)\n",
    "                mask = torch.ones(length)\n",
    "                if length < max_length:\n",
    "                    pad_length = max_length - length\n",
    "                    # Pad the input values\n",
    "                    val = torch.nn.functional.pad(val, (0, pad_length), value=0.0)\n",
    "                    # Extend the attention mask with zeros for padding\n",
    "                    mask = torch.nn.functional.pad(mask, (0, pad_length), value=0.0)\n",
    "\n",
    "                padded_input_values.append(val)\n",
    "                attention_mask.append(mask)\n",
    "\n",
    "            # Stack the padded inputs and attention masks\n",
    "            batch = {\n",
    "                \"input_values\": torch.stack(padded_input_values),\n",
    "                \"attention_mask\": torch.stack(attention_mask)\n",
    "            }\n",
    "\n",
    "            # Get labels\n",
    "            if \"labels\" in features[0]:\n",
    "                labels = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "                # Pad labels manually with -100 (ignore index for CTC loss)\n",
    "                padded_labels = []\n",
    "                max_label_length = max(len(l) for l in labels)\n",
    "\n",
    "                for label in labels:\n",
    "                    if isinstance(label, torch.Tensor):\n",
    "                        label = label.tolist()\n",
    "\n",
    "                    if len(label) < max_label_length:\n",
    "                        # Pad with -100\n",
    "                        label = label + [-100] * (max_label_length - len(label))\n",
    "\n",
    "                    padded_labels.append(torch.tensor(label, dtype=torch.long))\n",
    "\n",
    "                batch[\"labels\"] = torch.stack(padded_labels)\n",
    "\n",
    "            return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1470fb65-bb60-42e8-b7ac-f98f4ed742f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = SimpleCTCDataCollator(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f951c6c2-acf9-4313-8d08-5051688c3030",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# This is the version that doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a752fff6-9ae0-4875-b307-299b8c477917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=processor.tokenizer, padding=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee78054-7d3d-4366-ac84-dde99b44e2b6",
   "metadata": {},
   "source": [
    "# Continue\n",
    "\n",
    "The next line of code is for automatically generating a tokenizer, it is better to use the manual vocab because this let's you overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f50929e-e0c8-4134-980e-3ec4d4a24018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./tokenizer_files/vocab.json', './tokenizer_files/merges.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# # train tokenizer on your dataset text files\n",
    "# tokenizer.train(files=[\"./tokenizer_files/creolese_corpus.txt\"], vocab_size=1000, min_frequency=2, special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
    "\n",
    "# tokenizer.save_model(\"./tokenizer_files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "815a0e11-8ffc-4046-9684-81abdda296d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-creolese-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=20,\n",
    "    dataloader_num_workers=0,\n",
    "    logging_steps=5,\n",
    "    save_steps=2,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89cc2afa-4002-4fba-8bb0-36f7a6af925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "import torch\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = torch.argmax(torch.tensor(pred_logits), dim=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = jiwer.wer(label_str, pred_str)\n",
    "    mer = jiwer.mer(label_str, pred_str)\n",
    "    cer = jiwer.cer(label_str, pred_str)\n",
    "    return {\"wer\": wer, \"mer\": mer, \"cer\": cer}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "987bf2f4-e10b-4ee4-805f-4a41498411d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_146437/3511990523.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=SimpleCTCDataCollator(processor=processor),\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=processed_dataset,\n",
    "    eval_dataset=processed_dataset,\n",
    "    tokenizer=processor.feature_extractor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4016ccd-3b4f-4c57-a03d-622e14cc6cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 1:15:56, Epoch 16/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>11.764800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.904400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.784900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.521100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.020300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.425400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.304800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.890100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.846100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.885200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.368500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.855500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.858800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.850700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.831000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.857900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>3.365800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.837200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.3533787727355957,\n",
       " 'eval_wer': 0.9902642559109874,\n",
       " 'eval_mer': 0.9902642559109874,\n",
       " 'eval_cer': 0.9993579295698128,\n",
       " 'eval_runtime': 72.9312,\n",
       " 'eval_samples_per_second': 0.288,\n",
       " 'eval_steps_per_second': 0.041,\n",
       " 'epoch': 16.761904761904763}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e521eb4b-43fe-4356-afff-7e17291bdf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[NeMo I 2025-05-06 10:30:00 trainer:600] Epoch 1: train_loss=5.12, val_wer=0.72]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n[NeMo I 2025-05-06 10:30:00 trainer:600] Epoch 1: train_loss=5.12, val_wer=0.72]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7af321de-0f77-410a-ba8c-87a69999b867",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m pred_text = processor.batch_decode(pred_ids)[\u001b[32m0\u001b[39m]\n\u001b[32m     10\u001b[39m true_text = sample[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m decoded_label = processor.batch_decode([\u001b[43mlabels\u001b[49m], group_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrediction: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReference: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoded_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "# manually load an eval sample\n",
    "sample = processed_dataset[0]  # replace 0 with any index\n",
    "audio_input = processor(sample[\"input_values\"], return_tensors=\"pt\", sampling_rate=16000)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(audio_input.input_values.to(device)).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "pred_text = processor.batch_decode(pred_ids)[0]\n",
    "true_text = sample[\"labels\"]\n",
    "\n",
    "decoded_label = processor.batch_decode([labels], group_tokens=False)[0]\n",
    "print(f\"Prediction: '{pred_text}'\")\n",
    "print(f\"Reference: '{decoded_label}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7b60d5d-1a67-4062-9c29-ffa67a3d6d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./wav2vec2-creolese-finetuned\")\n",
    "processor.save_pretrained(\"./wav2vec2-creolese-finetuned\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
