{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fvGndVI8NIC"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.24.4\n",
        "!pip install nemo_toolkit --no-cache-dir\n",
        "!pip install torch torchaudio\n",
        "!pip install pyannote.metrics datasets webdataset tqdm braceexpand hydra-core omegaconf lightning lhotse jiwer pyannote.core\n",
        "!pip install einops sentencepiece\n",
        "!pip install editdistance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJtkrpBP8QvN"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive (remove if running locally)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Set paths\n",
        "# BASE_PATH = \"/content/drive/MyDrive/creole_asr_project\"\n",
        "\n",
        "# Data paths\n",
        "# AUDIO_DIR = f\"{BASE_PATH}/data/audio\"\n",
        "# FINETUNE_DIR = f\"{BASE_PATH}/data/finetune_eligible\"\n",
        "# TRANSCRIPTS_DIR = f\"{BASE_PATH}/data/transcripts\"\n",
        "# MANIFESTS_DIR = f\"{BASE_PATH}/data/manifests\"\n",
        "\n",
        "# Model paths\n",
        "# PRETRAINED_MODEL_DIR = f\"{BASE_PATH}/models/pretrained\"\n",
        "# CHECKPOINT_DIR = f\"{BASE_PATH}/models/checkpoints\"\n",
        "# FINAL_MODEL_DIR = f\"{BASE_PATH}/models/final\"\n",
        "\n",
        "# Create directories\n",
        "# !mkdir -p \"{AUDIO_DIR}\" \"{TRANSCRIPTS_DIR}\" \"{MANIFESTS_DIR}\"\n",
        "# !mkdir -p \"{PRETRAINED_MODEL_DIR}\" \"{CHECKPOINT_DIR}\" \"{FINAL_MODEL_DIR}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwZaMTSXFNwT"
      },
      "outputs": [],
      "source": [
        "# Set paths (local version)\n",
        "# Uncomment this stuff if running on actual hardware or cloud system other than Google Colab\n",
        "BASE_PATH = \"creolese-audio-dataset\"  # Base folder containing the dataset. Change to whatever you want\n",
        "\n",
        "Data paths (local version)\n",
        "AUDIO_DIR = f\"{BASE_PATH}/Audio Files\"\n",
        "FINETUNE_DIR = f\"{AUDIO_DIR}/finetune_eligible\"\n",
        "TRANSCRIPTS_DIR = BASE_PATH  # Transcripts are at base level\n",
        "MANIFESTS_DIR = f\"{BASE_PATH}/manifests\"\n",
        "\n",
        "# Model paths (local version)\n",
        "PRETRAINED_MODEL_DIR = f\"{BASE_PATH}/models/pretrained\"\n",
        "CHECKPOINT_DIR = f\"{BASE_PATH}/models/checkpoints\"\n",
        "FINAL_MODEL_DIR = f\"{BASE_PATH}/models/final\"\n",
        "\n",
        "# Create directories\n",
        "import os\n",
        "os.makedirs(MANIFESTS_DIR, exist_ok=True)\n",
        "os.makedirs(PRETRAINED_MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(FINAL_MODEL_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import librosa\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from nemo.collections.asr.models import EncDecHybridRNNTCTCBPEModel\n",
        "from omegaconf import OmegaConf, open_dict\n",
        "from lightning.pytorch import Trainer\n",
        "import lightning.pytorch as pl\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint, Callback\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "import torch\n",
        "\n",
        "def normalize_audio_filename(filename):\n",
        "    \"\"\"Normalize filenames for consistent matching...ex 4a.wav, 4b.wav etc.\n",
        "    Haven't tested to see if this works\n",
        "    \"\"\"\n",
        "    base, ext = os.path.splitext(filename)\n",
        "    if ext.lower() != '.wav':\n",
        "        return None, None\n",
        "    return filename, re.sub(r'[^a-zA-Z0-9]', '', base).lower()\n",
        "\n",
        "def create_manifests_from_finetune(\n",
        "    audio_dir: str,\n",
        "    finetune_dir: str,\n",
        "    output_train_path: str,\n",
        "    output_val_path: str,\n",
        "    val_split: float = 0.2,\n",
        "    min_duration: float = 1.0,\n",
        "    max_duration: float = 40.0,\n",
        "    sample_rate: int = 16000,\n",
        "    default_language: str = \"en\"\n",
        ") -> None:\n",
        "    \"\"\"Create train/val manifests with robust audio-transcript matching.\"\"\"\n",
        "    random.seed(42)\n",
        "\n",
        "    # Load transcripts\n",
        "    try:\n",
        "        with open(Path(finetune_dir)/\"transcripts.json\", 'r') as f:\n",
        "            transcript_entries = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: transcripts.json not found in {finetune_dir}\")\n",
        "        return\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Could not decode JSON from {Path(finetune_dir)/'transcripts.json'}\")\n",
        "        return\n",
        "\n",
        "    # Create mappings\n",
        "    audio_files = {}\n",
        "    try:\n",
        "        for f in os.listdir(audio_dir):\n",
        "            if f.lower().endswith('.wav'):\n",
        "                original, normalized = normalize_audio_filename(f)\n",
        "                if normalized:\n",
        "                    audio_files[normalized] = original\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Audio directory not found at {audio_dir}.\")\n",
        "        return\n",
        "\n",
        "    transcript_map = {}\n",
        "    for entry in transcript_entries:\n",
        "        original_name = entry.get('audio', '')\n",
        "        if original_name:\n",
        "            _, normalized = normalize_audio_filename(original_name)\n",
        "            if normalized:\n",
        "                entry['language'] = entry.get('language', \"en\")\n",
        "                entry['lang'] = entry.get('lang', \"en\")\n",
        "                entry['text'] = entry.get('text', \"\")\n",
        "                transcript_map[normalized] = entry\n",
        "\n",
        "    # Match audio-transcript pairs\n",
        "    matched_entries = []\n",
        "    for norm_name, audio_file in audio_files.items():\n",
        "        if norm_name in transcript_map:\n",
        "            entry = transcript_map[norm_name]\n",
        "            audio_path = os.path.join(audio_dir, audio_file)\n",
        "\n",
        "            if not os.path.exists(audio_path):\n",
        "                print(f\"Skipping {audio_file}: Audio file not found.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                duration = librosa.get_duration(filename=audio_path)\n",
        "                if min_duration <= duration <= max_duration:\n",
        "                    matched_entries.append({\n",
        "                        'audio_filepath': audio_path,\n",
        "                        'text': entry['text'],\n",
        "                        'duration': duration,\n",
        "                        'language': entry['language'],\n",
        "                        'lang': entry['lang'],\n",
        "                        'sample_rate': sample_rate\n",
        "                    })\n",
        "                else:\n",
        "                    print(f\"Skipping {audio_file}: Duration outside range\")\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping {audio_file}: Error getting duration - {str(e)}\")\n",
        "\n",
        "    if not matched_entries:\n",
        "        print(\"No valid audio-transcript pairs found.\")\n",
        "        return\n",
        "\n",
        "    # Split and write manifests\n",
        "    random.shuffle(matched_entries)\n",
        "    split_idx = int(len(matched_entries) * (1 - val_split))\n",
        "\n",
        "    def _write_manifest(entries, path):\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        with open(path, 'w') as f:\n",
        "            for entry in entries:\n",
        "                json.dump(entry, f)\n",
        "                f.write('\\n')\n",
        "\n",
        "    _write_manifest(matched_entries[:split_idx], output_train_path)\n",
        "    _write_manifest(matched_entries[split_idx:], output_val_path)\n",
        "\n",
        "    print(f\"Created manifests: {len(matched_entries[:split_idx])} training, {len(matched_entries[split_idx:])} validation samples\")"
      ],
      "metadata": {
        "id": "sscir7N-EeCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchedModel(EncDecHybridRNNTCTCBPEModel):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.validation_step_outputs = []  # Store validation outputs here\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"Override validation step to capture outputs and handle WER safely\"\"\"\n",
        "        output = super().validation_step(batch, batch_idx)\n",
        "\n",
        "        # Calculate WER immediately and store safe value\n",
        "        if hasattr(self, '_wer'):\n",
        "            try:\n",
        "                wer = self._wer.compute()\n",
        "                if wer is not None:\n",
        "                    # Ensure WER is never negative\n",
        "                    safe_wer = max(float(wer), 0.0) if isinstance(wer, (int, float)) else wer.clamp(min=0)\n",
        "                    output['val_wer'] = safe_wer\n",
        "                    self.log(\"val_wer\", safe_wer, prog_bar=True, on_step=False, on_epoch=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error comupting WER: {str(e)}\")\n",
        "                output['val_wer'] = torch.tensor(0.0)  # Default safe value\n",
        "\n",
        "        self.validation_step_outputs.append(output)\n",
        "        return output\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        \"\"\"Clean up stored outputs\"\"\"\n",
        "        self.validation_step_outputs = []\n",
        "\n",
        "        # Call parent's validation logic if needed\n",
        "        if hasattr(super(), 'on_validation_epoch_end'):\n",
        "            super().on_validation_epoch_end()\n",
        "\n",
        "\n",
        "def configure_and_train_model():\n",
        "    \"\"\"Configure and train the ASR model with proper settings\"\"\"\n",
        "    # Create directories if they don't exist\n",
        "    os.makedirs(MANIFESTS_DIR, exist_ok=True)\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "    os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "    # Create manifests\n",
        "    train_manifest_path = f\"{MANIFESTS_DIR}/train_manifest.json\"\n",
        "    val_manifest_path = f\"{MANIFESTS_DIR}/val_manifest.json\"\n",
        "\n",
        "    create_manifests_from_finetune(\n",
        "        audio_dir=FINETUNE_DIR,\n",
        "        finetune_dir=FINETUNE_DIR,\n",
        "        output_train_path=train_manifest_path,\n",
        "        output_val_path=val_manifest_path,\n",
        "        val_split=0.2,\n",
        "        min_duration=1.0,\n",
        "        max_duration=40.0,\n",
        "        sample_rate=16000\n",
        "    )\n",
        "\n",
        "    # Check manifests\n",
        "    if not os.path.exists(train_manifest_path) or not os.path.exists(val_manifest_path):\n",
        "        print(\"Manifest files were not created.\")\n",
        "        return\n",
        "    if os.path.getsize(train_manifest_path) == 0 or os.path.getsize(val_manifest_path) == 0:\n",
        "        print(\"Manifest files are empty.\")\n",
        "        return\n",
        "\n",
        "    # Initialize trainer\n",
        "    accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
        "    devices = 1\n",
        "\n",
        "    trainer = Trainer(\n",
        "        accelerator=accelerator,\n",
        "        devices=devices,\n",
        "        max_epochs=10,\n",
        "        enable_checkpointing=True,\n",
        "        logger=TensorBoardLogger(save_dir=\"logs\", name=\"creole_finetune\", log_model=False),\n",
        "        callbacks=[\n",
        "            ModelCheckpoint(\n",
        "                dirpath=CHECKPOINT_DIR,\n",
        "                save_top_k=1,\n",
        "                monitor=\"val_wer\",\n",
        "                mode=\"min\",\n",
        "                filename='best_model-{epoch}-{val_wer:.2f}',\n",
        "                save_last=True\n",
        "            )\n",
        "        ],\n",
        "        enable_progress_bar=True,\n",
        "        check_val_every_n_epoch=1,\n",
        "        num_sanity_val_steps=2\n",
        "    )\n",
        "\n",
        "    # Load model with our patched version\n",
        "    model = PatchedModel.from_pretrained(\n",
        "        \"stt_multilingual_fastconformer_hybrid_large_pc\",\n",
        "        trainer=trainer\n",
        "    )\n",
        "\n",
        "    # Configure model\n",
        "    with open_dict(model.cfg):\n",
        "        model.cfg.train_ds = {\n",
        "            'manifest_filepath': train_manifest_path,\n",
        "            'sample_rate': 16000,\n",
        "            'batch_size': 4,\n",
        "            'shuffle': True,\n",
        "            'num_workers': 2,\n",
        "            'pin_memory': True,\n",
        "            'min_duration': 1.0,\n",
        "            'max_duration': 40.0,\n",
        "            'normalize_transcripts': True,\n",
        "            'trim_silence': True\n",
        "        }\n",
        "\n",
        "        model.cfg.validation_ds = {\n",
        "            'manifest_filepath': val_manifest_path,\n",
        "            'sample_rate': 16000,\n",
        "            'batch_size': 4,\n",
        "            'shuffle': False,\n",
        "            'num_workers': 2,\n",
        "            'pin_memory': True,\n",
        "            'normalize_transcripts': True,\n",
        "            'trim_silence': True,\n",
        "            'return_sample_id': False\n",
        "        }\n",
        "\n",
        "        model.cfg.optim = {\n",
        "            'lr': 0.0001,\n",
        "            'sched': {'name': 'CosineAnnealing', 'warmup_steps': 1000}\n",
        "        }\n",
        "        model.cfg.language = \"en\"\n",
        "\n",
        "    # Setup training\n",
        "    model.setup_training_data(train_data_config=model.cfg.train_ds)\n",
        "    model.setup_validation_data(val_data_config=model.cfg.validation_ds)\n",
        "\n",
        "    # Train with error handling\n",
        "    try:\n",
        "        trainer.fit(model)\n",
        "        model.save_to(f\"{FINAL_MODEL_DIR}/creole_english_finetuned.nemo\")\n",
        "        print(\"Training complete and model saved!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Training failed: {e}\")\n",
        "        try:\n",
        "            model.save_to(f\"{FINAL_MODEL_DIR}/partial_model.nemo\")\n",
        "            print(\"Saved partial model\")\n",
        "        except Exception as save_error:\n",
        "            print(f\"Failed to save partial model: {save_error}\")\n",
        "        raise\n",
        "\n",
        "configure_and_train_model()"
      ],
      "metadata": {
        "id": "uM1s7Nxqa5d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37iWT4q6y78p"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiCRqFVE7lRL"
      },
      "outputs": [],
      "source": [
        "# Optional: Resume from best checkpoint\n",
        "checkpoints = !ls \"{CHECKPOINT_DIR}\" | grep .ckpt\n",
        "if checkpoints:\n",
        "    best_ckpt = f\"{CHECKPOINT_DIR}/{checkpoints[0]}\"\n",
        "    trainer.fit(model, ckpt_path=best_ckpt)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}